 This is gonna blow your mind these results are really surprising So this graph is where we start it shows all of these big popular large language models that are the best ones we currently have You've got of course GPT 3.5. You've got GPT 4 from OpenAI Grok from Twitter. You've got Mixtral, which is great. Mixtral is great. Llama is Facebook And then DBRX is from Databricks Okay, you can see them down here at the bottom and then the Claude series Is from Anthropic so these are all the big ones and some of these are open source and some of these are not But the really exciting thing about this is how these performance metrics compare and what sets some of them apart So one of the things first of all that you can see here is these are the three most popular Performance metrics that anyone can run on any of these so MML you the orange one is showing like word problems based on undergraduate level like general knowledge about the world, okay? How much does it generally know about the world? GSM 8k is Grade school level math problems, so the the gray one right hello swag is like does it understand the way people actually talk? Right beyond like the rules of grammar is able to understand Sort of like more idiomatic language, and then the blue one here is the average of those three But the thing that jumped out at me immediately is like they all basically have the same Yellow one right so let's take that out So here you can kind of see like better like okay. There's a clear trend here. There are some big differences and especially the difference between Like the trend of the gray and the trend of the orange like right here. There is kind of a line these ones have way better ability to do math problems than these ones and The reason is because these are not doing the math problems. These are all closed source models that they taught how to They taught them that they're not good at math and they shouldn't try and instead they should write API calls to something like wolfram alpha in the case of GPT-4 and then it's gonna it knows how to write the problem and then the Separate API will do the problem and give it the answer So these is kind of cheating like the score is way better because they're not really doing the math These ones are actually doing the math and AI is not very good at doing math, okay? but the thing that is really interesting to me here is there's a subtle distinction between a lot of these which is whether they are single expert or mixture of experts and I wanted to see like how can we how can we show that and how can we how can we show the difference between the two? Also, some of these are crazy big and some of these can run on your phone right so like how do we derive third variables for relationships for example between the size and how good it is right so like if there's a Model that I need a data center to run and it's okay at stuff Versus one that runs on my phone that is good at stuff. I want to see that right I want to see the relationship between how big it is how hard it is to run it how much it costs to run it and how good it is at stuff so what if we divide the active parameters divide these metrics by the active parameters and derive a third variable and graph that and we could do it separately for word problems and math problems so that looks like this so again the orange here is the math problems the blue is the word problems and what we see is there's this like really really big difference so these ones they don't actually report the number of parameters so let's ignore those because we can't really compare but we know they're huge and they run in data centers so they're kind of not what we're looking for right so for these ones grok is terrible right predictably all of their competent AI people quit to go work at open AI and work on things like GPT-4 which is also terrible right compare that instead to even GPT-3.5 performs better in the ratio of these metrics to the number of parameters so GPT-4 is like a thousand times bigger than GPT-3.5 and it's not that much better at this stuff there is this asymptote in the relationship right you make a model bigger it gets better up to a point and then if you make it bigger it doesn't get better anymore it kind of levels off and it just gets more expensive and that's GPT-4 right it's kind of surprising because when you when you map it this way when you divide them this way you see it's actually not better for how much it costs it's significantly worse for how much it costs it'll give you a better answer but it'll cost a lot more right so then we move up the up the ladder here you get to llama okay llama is pretty big pretty new right pretty good at stuff but it's not using the modern architecture it's not mixture of experts it's single expert and that gives it a huge disadvantage against all these other ones right and so what you see up here with the ones that are the absolute best is that they have these very complex mixture of experts architectures and it allows them to do really really really good results very small so you can run it on your computer potentially on your phone depending on it like I don't know how much RAM people have on their phones these days but these are actually pretty small and crazy good for how small they are and that ratio they're way better than even these very large models like GPT-4 so this is a super interesting graph and it tells us a lot about what we can expect in the future we can expect smaller more complex denser models that compact a lot more knowledge into a small space one of the big things that I took away from Mark Zuckerberg's recent interview on llama3 is that they decided at some point like okay we're just gonna stop training it it hasn't hit the asymptote yet like it's still learning but at some point like we have other things to do and my question then it's like well what's the limit how much can you fit into these models with these new architectures can you can you 10x it like could it be could it be up here like if they would have let it keep going and we don't know but I think that's the big question that people are gonna be asking you also have the fact that like ternary quantization for parameters is going to drastically reduce the performance cost and then we'll have to think of how to how to derive a variable for that right it's very interesting and there's a lot going on right now but it's very interesting especially to see this incredible difference between these big expensive models and these small much better models and I've never seen a graph like this before so I think is a very cool way to think about it.